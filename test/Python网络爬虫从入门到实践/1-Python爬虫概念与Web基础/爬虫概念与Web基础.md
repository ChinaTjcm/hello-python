# Python 爬虫概念与 Web 基础

## 一、爬虫概念

### 1.1 什么是爬虫

       爬虫，即网络爬虫，又称网络蜘蛛（Web Spider），是一种按照一定规则，用来自动浏览或抓取万维网数据的程序。可以把爬虫程序看成一个机器人，它的功能就是模拟人的行
    为去访问各种站点，或者带回一些与站点相关的信息。它可以 24 小时不间断地做一些重复
    性的工作，还可以自动提取一些数据。
       有一点要注意，本章标题虽然写明 Python 爬虫，但并不是只有 Python 才能编写爬虫
    程序。其他的编程语言也可以用来编写爬虫程序，如 PHP 有 phpspider 爬虫框架，Java 有
    WebMagic 爬虫框架，C#有 DotnetSpider 爬虫框架等。

### 1.2 场景

    看需求...

### 1.3 爬虫组成部分

    大概分为：
        模拟请求
        数据解析
        数据保存

### 1.4 模拟请求

伪装像人一样去访问互联网站点

    1. 最简单站点，什么都不处理，发送请求，给出响应结果
    2. 稍微复杂一点的站点，会判断请求头中的 User-Agent 是否为浏览器请求，Host 字
    段是否为正确的服务器域名，以及 Referer 字段的地址是否合法。
    3. 再复杂一点的站点，需要登录后才能访问。登录后会持有一个 Cookie 或 Session
    会话，你需要带着这个东西才能执行一些请求，否则都会跳到登录页。
    4. 更复杂一点的站点，登录很复杂，需要五花八门的验证码、最简单的数字图片加
    噪点、滑动验证码、点触验证码。除此之外，还有一些其他特立独行的验证方式，
    如最经典的微博宫格验证码、极验验证码的行为验证等。  
    5. 更复杂的站点，其链接和请求参数都是加密的，需要研究、破解加密规则才
    能够模拟访问。
    6. 可能存在的反爬虫套路，限制 ip 访问频次、JavaScript 动态加载数据等。

### 1.5 数据解析

数据解析是对模拟请求获得的不同类型的信息进行解析。

    1. 返回的结果是 HTML 或 XML，可利用 Beautiful Soup、Xpath、PyQuery 等模块解
    析需要的节点数据。
    2. 返回的是 JSON 字符串或其他字符串，可通过编写正则表达式提取所需信息。
    3. 返回的是加密后的数据，则需要解密后才能解析。

### 1.6 数据保存

数据保存是对解析后的数据进行保存。如果把采集到的数据放在内存里，一旦关
闭程序，数据就丢失了，因此我们需要把爬取到的数据保存到本地。

    保存的形式有以下几种：
         保存为文本文件。一些文字类型的信息，如小说内容，可保存成 TXT 文件。
         保存为图片、音视频等二进制文件。如一些多媒体资源可保存为这种格式。
         保存到 Excel 表格中。其好处是直观，而且方便不了解编程的读者使用。
         保存到数据库中。数据库又分为关系型数据库和非关系型数据库。

## 二、HTTP 简述

## 三、


