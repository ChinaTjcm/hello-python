# Python 爬虫概念与 Web 基础

## 一、爬虫概念

### 1.1 什么是爬虫

       爬虫，即网络爬虫，又称网络蜘蛛（Web Spider），是一种按照一定规则，用来自动浏览或抓取万维网数据的程序。可以把爬虫程序看成一个机器人，它的功能就是模拟人的行
    为去访问各种站点，或者带回一些与站点相关的信息。它可以 24 小时不间断地做一些重复
    性的工作，还可以自动提取一些数据。
       有一点要注意，本章标题虽然写明 Python 爬虫，但并不是只有 Python 才能编写爬虫
    程序。其他的编程语言也可以用来编写爬虫程序，如 PHP 有 phpspider 爬虫框架，Java 有
    WebMagic 爬虫框架，C#有 DotnetSpider 爬虫框架等。

### 1.2 场景

    看需求...

### 1.3 爬虫组成部分

    大概分为：
        模拟请求
        数据解析
        数据保存

### 1.4 模拟请求

伪装像人一样去访问互联网站点

    1. 最简单站点，什么都不处理，发送请求，给出响应结果
    2. 稍微复杂一点的站点，会判断请求头中的 User-Agent 是否为浏览器请求，Host 字
    段是否为正确的服务器域名，以及 Referer 字段的地址是否合法。
    3. 再复杂一点的站点，需要登录后才能访问。登录后会持有一个 Cookie 或 Session
    会话，你需要带着这个东西才能执行一些请求，否则都会跳到登录页。
    4. 更复杂一点的站点，登录很复杂，需要五花八门的验证码、最简单的数字图片加
    噪点、滑动验证码、点触验证码。除此之外，还有一些其他特立独行的验证方式，
    如最经典的微博宫格验证码、极验验证码的行为验证等。  
    5. 更复杂的站点，其链接和请求参数都是加密的，需要研究、破解加密规则才
    能够模拟访问。
    6. 可能存在的反爬虫套路，限制 ip 访问频次、JavaScript 动态加载数据等。

### 1.5 数据解析

数据解析是对模拟请求获得的不同类型的信息进行解析。

    1. 返回的结果是 HTML 或 XML，可利用 Beautiful Soup、Xpath、PyQuery 等模块解
    析需要的节点数据。
    2. 返回的是 JSON 字符串或其他字符串，可通过编写正则表达式提取所需信息。
    3. 返回的是加密后的数据，则需要解密后才能解析。

### 1.6 数据保存

数据保存是对解析后的数据进行保存。如果把采集到的数据放在内存里，一旦关
闭程序，数据就丢失了，因此我们需要把爬取到的数据保存到本地。

    保存的形式有以下几种：
        1. 保存为文本文件。一些文字类型的信息，如小说内容，可保存成 TXT 文件。
        2. 保存为图片、音视频等二进制文件。如一些多媒体资源可保存为这种格式。
        3. 保存到 Excel 表格中。其好处是直观，而且方便不了解编程的读者使用。
        4. 保存到数据库中。数据库又分为关系型数据库和非关系型数据库。

## 二、HTTP 简述

在开始学习编写爬虫之前，我们还要了解与 Web 基础相关的一些内容，如 HTTP。

### 2.1 简述一次网络请求过程

在浏览器中输入 http://www.baidu.com 这个域名，然后回车。

1.通过 URL 查找服务器 IP

      浏览器先要做的是找出与域名对应的服务器，即把域名解析成对应服务器的 IP 地址。
    对于 www.baidu.com，浏览器并不认识这个域名，前面的 www 是服务器名，baidu 可以理
    解为公司名或私人名，最后的 com 则是域名根服务器。
      浏览器先访问本地 Host 文件，检查文件中是否有与域名匹配的 IP 地址，如果有则直
    接访问 IP 对应的服务器；否则，向上层的 DNS 服务器询问；如果还没有，则继续向上层DNS 服务器询问，直到 DNS 根服务器。

2.三次握手建立 TCP 连接

           获得服务器 IP 后，接下来是和服务器建立连接，这就是常说的 TCP 三次握手
        首先，客户端发送一个带 SYN 标志的数据包给服务器。服务器收到后，回传一个带
        SYN/ACK 标志的数据包表示信息确认。最后，客户端回传一个带 ACK 标志的数据包，表
        示握手结束，连接建立成功。

3.发送 HTTP 请求

           客户端和服务器建立连接后就可以开始发送 HTTP 请求了。
        浏览器发送请求行、请求头信息，还发送一个空行，代表请求头信息发送结束。
        如果是 Post 提交，还会提交请求体。

4.服务器响应请求

        Web 服务器解析用户请求并进行相关处理，最后把处理结果组装成响应报文，返回给客户端。

5.浏览器解析 HTML

        浏览器解析服务器返回的 HTML 代码，并请求 HTML 里用到的 CSS、JS、图片等资源。

6.页面渲染后呈现给用户

        渲染的顺序是从上到下，下载和渲染是同时进行的，页面加载完成显示在浏览器上。
        以上就是用户在浏览器输入一个地址并回车后，直到浏览器经历的大致流程，如果想
        更加详尽地了解整个过程，可以访问 https://github.com/skyline75489/what-happens-when-zh_CN

### 2.2 URI 和 URL

URI（Uniform Resource Identifier，统一资源标志符）标记一个网络资源，强调的是给资源命名；
URL 用地址标记一个网络资源，强调的是给资源定位。

        下面举个简单的例子帮读者区分。
        比如，你在某个技术沙龙上认识了一位技术高手，他自称 XXX 公司的技术总监，这
        个头衔就是 URI，但是只有这个头衔的话，你没有那么容易找到他，你还需要知道他所在
        公司的地址。深圳市南山区科技园 X 栋 X 楼/XXX 公司/技术总监/X 某，通过上面这串地址
        你就可以找到这个人。这个完整的地址就是 URL。
        而通过这个 URL 也可以知道 X 某是 XXX 公司的技术总监，它既能当作标记使用，又能当作地址使用，所以 URL 是 URI 的子集。
        URI 由下面三部分组成：
        1. 访问资源的命名机制；
        2. 存放资源的主机名；
        3. 资源本身的名称，用路径标识，着重强调资源。
        URL 由下面三部分组成：
        1. 协议（或称服务方式）；
        2. 存有该资源的主机 IP 地址（有时也包含端口号）；
        3.主机资源的具体地址（如目录、文件名等）。
        一般习惯性地把我们说的网址称为 URL。

### 2.3 HTTP 请求报文

HTTP（Hyper Text Transfer Protocol，超文本传输协议）是万维网服务器将超文本传输到本地浏览器的传送协议，基于 TCP/IP
通信协议来传递数据。HTTP 是无状态的，以此限制每次连接只处理一个请求。服务器在处理完客户端请求，并接收到客户端的应答后，即断开连接，这种方式的好处是节省传输时间。
当然，如果想保持连接，可以在请求首部字段中添加请求头 Connection: keep-alive，表明使用持久连接，或者通过 Cookie
这类方式间接地保存用户之前的 HTTP 通信状态。

HTTP 请求报文由四部分组成，依次是请求行、请求头、空行和请求正文。下面依次对这四部分进行介绍。

#### 1.请求行

它由请求方法、URL 和 HTTP 版本三个字段组成，使用空格进行分隔。比如访问百度，
请求行的内容为 GET/index.php HTTP/1.1。

HTTP/1.1 定义了八种请求方法：
方 法 描 述
GET 请求指定页面，并返回页面内容
POST 一般用于提交表单或上传数据，数据被包含在请求体中
PUT 客户端向服务器发送数据，以取代指定文档内容
DELETE 请求服务器删除指定页面
CONNECT HTTP/1.1 协议中预留给能够将连接改为管道方式的代理服务器
HEAD 类似于 GET 请求，只是返回的响应无具体内容，一般用于获取报头
OPTIONS 允许客户端查看服务器的性能
TRACE 回显服务器收到的请求，一般用于测试或诊断

在上述请求方法中，GET 和 POST 使用得最频繁。GET 请求无消息体，只能携带少量数据（最多只有 1024 字节），并将数据存放在 URL 地址中；而
POST 请求有消息体，可以携带大量数据，并将数据存放在消息体中。

URL 由 4 部分构成，协议 + 主机 + 路径 + 参数
例：http://www.baidu.com/login.html?username=cm&pwd=123

#### 2 请求头

它采用键值对的形式，关键字和值用英文冒号（:）进行分隔。
在实际开发过程中，请求头不限于上面这些内容，可以通过抓包的方式查看所用到的请求头，然后在编写爬虫时添加上。

百度百科全文：
https://baike.baidu.com/item/http%E8%AF%B7%E6%B1%82%E5%A4%B4/6623287?fr=aladdin

#### 3 空行

请求头的最后会有一个空行，表示请求头结束，接下来是请求正文，这个空行必不可少。

#### 4 请求正文

这一般是 POST 请求中提交的表单数据。另外，在请求头 Content-Type 中要使用正确
的类型才能正常提交，如表单数据是 application/x-www-form-urlencoded、文件是 multipart/
form-data、JSON 数据是 application/json、XML 数据是 text/xml，在抓包时可以获取。

### 2.4 HTTP 响应报文

HTTP 响应报文由四部分组成，依次是状态行、响应头、空行和响应正文。下面依次
对这四部分进行讲解。

1. 状态行 </br>

        它由协议版本、状态码、状态码描述三个字段组成，它们之间使用空格进行分隔。比
        如访问百度时，状态行的内容为 HTTP/1.1 200 OK。状态码分为下述五大类。
        1xx：指示信息，表示请求已接收，继续处理。
        2xx：成功，表示请求已被成功接收、理解。
        3xx：重定向，表示要完成请求必须进行更进一步的操作。
        4xx：客户端错误，表示请求有语法错误或请求无法实现。
        5xx：服务器端错误，表示服务器未能实现合法的请求。
        无须记忆，使用时查表即可。
2. 响应头 <br>
   服务器对请求的一些应答信息

3. 空行 <br>
   响应头的最后会有一个空行，表示响应头结束，接下来是响应正文。这个空行必不
   可少。
4. 响应正文 <br>
   Content-Type 指定响应正文的 Mime 类型，比如 text/html 类型响应正文为 HTML 代码，
   image/png 类型响应正文为 PNG 图片的二进制数据。

## 三、 网页的组成


## 四、

